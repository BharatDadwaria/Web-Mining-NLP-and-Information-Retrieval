{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Mining : Information Retrival and Natural Language Processing\n",
    "\n",
    "### Ranking document based on Cosine Similarity on real time CISI dataset\n",
    "Dataset link : https://www.kaggle.com/dmaso01dsta/cisi-a-dataset-for-information-retrieval\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Statment :\n",
    "Represent collection of documents in VSM (Vector Space Model) using tf and tf*idf (Preprocess using stopword\n",
    "removal/stemming). Create an Inverted Index for accessing the documents with respect to terms, use tf based and tf idf based weighting. For a query find similarity of query and\n",
    "documents using cosine similarity measures by using VSM representation(tf,tfidf) and on\n",
    "basis of Okapi Model. Display the ranked order of documents. Use standard benchmark\n",
    "data set CISI/CACM. Thus you have three models â€“ Tf, Tf-idf, okapi."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Step 1: Importing all the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#from nltk.stem import PorterStemmer\n",
    "#from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n",
    "import operator## Create Vocabulary\n",
    "import nltk\n",
    "from collections import Counter\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 : Creating Preprocessing functions\n",
    "* Converting all to lower case\n",
    "* Removing stop words\n",
    "* Removing Punctuations\n",
    "* Stemming\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting to lower case\n",
    "def lower_case(data):\n",
    "    return np.char.lower(data)\n",
    "\n",
    "def remove_stopwords(data):\n",
    "    stop_words=nltk.corpus.stopwords.words('english')\n",
    "    #print(stop_words)\n",
    "    new_data=\"\"\n",
    "    word_tokens=nltk.tokenize.word_tokenize(str(data))\n",
    "    for w in word_tokens:\n",
    "        if w not in stop_words:\n",
    "            new_data=new_data+\" \"+w\n",
    "    return new_data\n",
    "\n",
    "def remove_punctuations(data):\n",
    "    symbols = \"!\\\"#$%&()*+-./:;<=>?@[\\]^_`{|}~\\n\"\n",
    "    for i in range(len(symbols)):\n",
    "        data = np.char.replace(data, symbols[i], ' ')\n",
    "        data = np.char.replace(data, \"  \", \" \")\n",
    "    data = np.char.replace(data, ',', '')\n",
    "    return data\n",
    "\n",
    "def remove_apostrophe(data):\n",
    "    return np.char.replace(data, \"'\", \"\")\n",
    "\n",
    "def stemming(data):\n",
    "    stemmer= nltk.stem.PorterStemmer()\n",
    "    \n",
    "    tokens = nltk.tokenize.word_tokenize(str(data))\n",
    "    new_text = \"\"\n",
    "    for w in tokens:\n",
    "        new_text = new_text + \" \" + stemmer.stem(w)\n",
    "    return new_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=['My name is Bharat Dadwaria, I live in delhi and study at Jawaharlal Nehru University, New Delhi']\n",
    "\n",
    "def preprocessing_data(data):\n",
    "    data=lower_case(data)\n",
    "    data=remove_punctuations(data)\n",
    "    data=remove_apostrophe(data)\n",
    "    data=stemming(data)\n",
    "    #Once more we need to remove the punctuations\n",
    "    data=remove_punctuations(data)\n",
    "    data=remove_stopwords(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Importing the dataset and preprocessing it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of documents = 1460.\n",
      "\n",
      "The present study is a history of the DEWEY Decimal Classification.  The first edition of the DDC was published in 1876, the eighteenth edition in 1971, and future editions will continue to appear as needed.  In spite of the DDC's long and healthy life, however, its full story has never been told.  There have been biographies of Dewey that briefly describe his system, but this is the first attempt to provide a detailed history of the work that more than any other has spurred the growth of librarianship in this country and abroad. \n",
      "1460\n"
     ]
    }
   ],
   "source": [
    "with open('cisi-a/CISI.ALL') as f:\n",
    "    lines = \"\"\n",
    "    for l in f.readlines():\n",
    "        lines += \"\\n\" + l.strip() if l.startswith(\".\") else \" \" + l.strip()\n",
    "    lines = lines.lstrip(\"\\n\").split(\"\\n\")\n",
    "\n",
    "processed_text = []\n",
    "processed_title = []\n",
    "doc_set = {}\n",
    "doc_list=[]\n",
    "doc_id = \"\"\n",
    "doc_text = \"\"\n",
    "doc_auth=\"\"\n",
    "doc_title={}\n",
    "for l in lines:\n",
    "    if l.startswith(\".I\"):\n",
    "        doc_id = l.split(\" \")[1].strip()\n",
    "    elif l.startswith(\".T\"):\n",
    "        processed_title.append((str((l.lstrip(\" \")))))\n",
    "        #doc_title[doc_id] = doc_text.lstrip(\" \")\n",
    "        #doc_id=\"\"\n",
    "        #doc_text=\"\"\n",
    "        \n",
    "    elif l.startswith(\".A\"):\n",
    "        doc_auth = l.lstrip(\" \")\n",
    "    elif l.startswith(\".X\"):\n",
    "        processed_text.append((str(preprocessing_data(doc_text.lstrip(\" \")))))\n",
    "        #processed_text.append(word(str(preprocessing_data(doc_text.lstrip(\" \")))))\n",
    "        doc_set[doc_id] = doc_text.lstrip(\" \")\n",
    "        doc_id = \"\"\n",
    "        doc_text = \"\"\n",
    "    else:\n",
    "        doc_text += l.strip()[3:] + \" \" # The first 3 characters of a line can be ignored.\n",
    "\n",
    "# Print something to see the dictionary structure, etc.\n",
    "print(f\"Total Number of documents = {len(doc_set)}\" + \".\\n\")\n",
    "print(doc_set[\"1\"]) # note that the dictionary indexes are strings, not numbers. \n",
    "print(len(doc_set))  \n",
    "#print(processed_title)\n",
    "#print(processed_text)\n",
    "doc_list=doc_set.values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4 : Converting the text into Vector Space Model using Tf-Idf And Performing Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Lenght of the Vocabulary is :  1494\n"
     ]
    }
   ],
   "source": [
    "vocabulary = set(processed_text)\n",
    "for doc in processed_text:\n",
    "    vocabulary.update((doc))\n",
    "    #vocabulary = list(vocabulary)# Intializating the tfIdf model\n",
    "print(\"The Lenght of the Vocabulary is : \",len(vocabulary))\n",
    "tf_idf_vector=TfidfVectorizer(vocabulary=vocabulary,stop_words='english')\n",
    "tf_idf_tran=tf_idf_vector.fit_transform(processed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Converting the query to the Vector Space model using tfidf.\n",
    "def gen_vector_T(tokens):\n",
    "    Q = np.zeros((len(vocabulary)))    \n",
    "    x= tf_idf_vector.fit_transform(tokens)\n",
    "    #print(tokens[0].split(','))\n",
    "    for token in tokens[0].split(','):\n",
    "        #print(token)\n",
    "        try:\n",
    "            ind = vocabulary.index(token)\n",
    "            Q[ind]  = x[0, tfidf.vocabulary_[token]]\n",
    "        except:\n",
    "            pass\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def cosine_sim(a, b):\n",
    " #   cos_sim = (np.dot(a, b)/(np.linalg.norm(a,ord=None)*np.linalg.norm(b,ord=None)))\n",
    "  #  return cos_sim\n",
    "\n",
    "def cosine_sim(x, y):\n",
    "    return np.dot(x, y) / (np.sqrt(np.dot(x, x)) * np.sqrt(np.dot(y, y)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity_T(k, query):\n",
    "    preprocessed_query = preprocessed_query = re.sub(\"\\W+\", \" \", query).strip()\n",
    "    tokens = nltk.tokenize.word_tokenize(str(preprocessed_query))\n",
    "    d_cosines = []\n",
    "    #print(tokens)\n",
    "    query_vector = gen_vector_T(tokens)\n",
    "    #print((query_vector))\n",
    "    \n",
    "    for d in tf_idf_tran.A:\n",
    "        d_cosines.append(cosine_sim(query_vector, d))\n",
    "    #print((d))                \n",
    "    out = np.array(d_cosines).argsort()[-k:][::-1]\n",
    "    #print(d_cosines)\n",
    "    return(out)\n",
    "    '''\n",
    "    #print(\"\")\n",
    "    d_cosines.sort()\n",
    "    a = pd.DataFrame()\n",
    "    print(d_cosines)\n",
    "    for i,index in enumerate(out):\n",
    "        a.loc[i,'index'] = str(index)\n",
    "        #a.loc[i,'Subject'] = doc_list|['Subject'][index]\n",
    "    for j,simScore in enumerate(d_cosines[-k:][::-1]):\n",
    "        a.loc[j,'Score'] = simScore\n",
    "    return a\n",
    "    '''\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "query=\" history of the DEWEY Decimal Classification.  The first edition of the DDC was published in\"\n",
    "query1=\"What is the need for information consolidation, evaluation, and retrieval in scientific research?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Search documents are(Documents ID) : \n",
      "\n",
      " [1459  478  480  481  482  483  484  485  486  487]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "top_search=cosine_similarity_T(10,query)\n",
    "print(\"Top 10 Search documents are(Documents ID) : \\n\\n\",top_search)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6:  Fetching the top Result query documents data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "search result : 1 \n",
      "Doc  1460  :  .T Modern Integral Information Systems for Chemistry and Chemical Technology \n",
      "\n",
      "\n",
      "This book considers the basic aspects of this complex problem - the historical and social essence of language and thought, their interaction in historical evolution, the essence of linguistic meaning in relation to the content side of thought, and the physiological mechanism of the processes of abstraction, generalization, etc. \n",
      "\n",
      "search result : 2 \n",
      "Doc  479  :  .T Automatic Term Classifications and Retrieval \n",
      "\n",
      "\n",
      "All analysis of information for storage and of questions for effecting retrieval must be in terms of concepts and the relations between them. The concepts may be just words (descriptors), as in simple post-co-ordinate keyword indexing systems, or they may be class-terms or other idea-groupings, as in classifications.  The relations between concepts often appear to be absent, but if more than one word is used in indexing or in a search there is clearly an implicit relation between them in the mind of the indexer or questioner, and other relations possible between the words would lead to false drops.  Classification has traditionally been a method of organization of concepts in which the relations between concepts are ostensibly displayed in the form of groupings called classes. \n",
      "\n",
      "search result : 3 \n",
      "Doc  481  :  .T Integrated Information Processing and the Case for a National Network \n",
      "\n",
      "\n",
      "An articulated subject index is one in which logical transformations of natural language phrases containing prepositions or connectives are employed to organize the noun phrases as subject headings, with subordinate displays of the remainder of the phrases.. The best-known example of the articulated subject index is that to Chemical Abstracts.. It has been shown that a well-defined transformation links the entry, as it appears in the index, with the original phrase, in natural order, as it was first compiled by the indexer.. A reverse transformation can be used to generate potential index entries from indexing phrases containing one or more prepositions or connectives.. A simple model has been devised for the generation of articulated subject index entries from natural language indexing phrases which exclude infinitives or words acting as gerunds or participles.. A sorting algorithm has also been developed, the purpose of which is to select those entries which lead to greatest organization in the index display.. Deviations from the model in manually-produced indexes are described.. The potential value of certain of these characteristics in information retrieval is examined.. \n",
      "\n",
      "search result : 4 \n",
      "Doc  482  :  .T Some Experiments in the Selective Dissemination of Information in the Field of Plasma Physics \n",
      "\n",
      "\n",
      "The various premises, which need consideration when developing a realistic and flexible information storage, retrieval and dissemination (ISRD) system, are discussed; their implication is illustrated with some examples from the development of the system at \"Shell\" Research, Sittingbourne.. One of the factors which will affect the satisfactory performance of an ISRD system is the ease with which relevant literature information not held in the system can be provided.. The later part of the paper is developed to a discussion of this problem and of a possible means of dealing with it in the not too distant future.. \n",
      "\n",
      "search result : 5 \n",
      "Doc  483  :  .T Performance of Automatic Information Systems \n",
      "\n",
      "\n",
      "A small-scale, computer-based SDI system in plasma physics and the related subjects is described briefly.. The system serves about 100 research scientists and engineers and uses title input only in order to minimize input costs.. The implications of this approach and its effect upon the system parameters is discussed.. Some comparison of the costs of the computer-based system with those of a manual system is made.. Further experiments are described in which the service is expanded to external users on a world-wide basis, the aim being to compare, under controlled conditions, the parameters of the small-scale internal service with those of an external service on a wide scale.. The paper concludes with some observations on the future development and organization of computer-assisted services, their possibilities and the main problems which are likely to arise.. \n",
      "\n",
      "search result : 6 \n",
      "Doc  484  :  .T Negotiation of Inquiries in an On-Line Retrieval System \n",
      "\n",
      "\n",
      "The SMART document retrieval system is used to investigate algorithms for text analysis and request searching.. Results from three document collections indicate that word normalization is efficiently performed by automatic thesaurus lookup, while phrase matching procedures, statistical association methods, and concept hierarchies are useful for special applications.. Automatic document clustering schemes and use-interactive feedback methods permit rapid searches of large collections.. Abstracts are found to be superior to titles as a base for content analysis in a document retrieval system and almost as good as complete texts.. Proper procedures for designing dictionaries and searching requests are discussed..The practicality of large scale document centers and their proper design are considered in light of these results.. \n",
      "\n",
      "search result : 7 \n",
      "Doc  485  :  .T A Clustering Experiment: First Step Towards a Computer-Generated Classification Scheme \n",
      "\n",
      "\n",
      "The focus of discussion is a prototype retrieval system with three major components for text processing, connectivity and decision operations.. Each of these components is based on a distinguishable subtheory.. Computer programs for the first two components have been written for a GE 225 computer.. The complete prototype system is now being programmed for operation in a time-shared environment.. It is a user-oriented system, with planned capabilities for the browsing and man-machine interaction.. A major goal is to develop procedures whereby research workers can conduct an on-line dialog via terminals with a body of scientific information.. Each user-submitted inquiry is a set of sentences without restriction as to vocabulary or form.. The system converses with the user to obtain source-derived phrases that elaborate and refine the initial inquiry.. The use is led to browse in the general area of his inquiry and to broaden or narrow it as a further aid to request formulation.. Evaluation of system performance is described.. \n",
      "\n",
      "search result : 8 \n",
      "Doc  486  :  .T Relevance Assessments and Retrieval System Evaluation \n",
      "\n",
      "\n",
      "A document collection consisting of 240 articles on theoretical high energy physics is analyzed by an empirical clustering procedure, in which bibliographic coupling, obtained by computer, is used to measure the relatedness of articles.. Meaningful groups of documents were produced.. The clustering process ia adapted to future use in the computer-generation of a classification scheme.. \n",
      "\n",
      "search result : 9 \n",
      "Doc  487  :  .T An Indirect Method of Information Retrieval \n",
      "\n",
      "\n",
      "Two widely used criteria for evaluating the effectiveness of information retrieval system are, respectively, the recall and the precision.. Since the determination of these measures is dependent on a distinction between documents which are relevant to a given query and documents which are not relevant to that query, it has sometimes been claimed that an accurate, generally valid evaluation cannot be based on recall and precision measures.. A study was made to determine that effect of variations in relevance assessments on the average recall and precision values used to measure retrieval effectiveness.. Using a collection of 1200 documents in information science for test purposes, it is found that large scale differences in the relevance assessments do not produce significant variations in average recall and precision.. It thus appears that properly computed recall and precision data may represent effectiveness indicators which are generally valid for many distinct user classes.. \n",
      "\n",
      "search result : 10 \n",
      "Doc  488  :  .T The Use of Automatically-Obtained Keyword Classifications for Information Retrieval \n",
      "\n",
      "\n",
      "The information retrieval process, treated strictly as a matching procedure, has the defects that tha whole file must be probed for each query, and that it overlooks the fact that the relevance of the information from one document depends upon what is already known about the subject, and in term affects the relevance of other documents subsequently examined.. A mathematical model of a search technique in which the defects of the direct method are taken into account is demonstrated by an experiment in which a given paper is treated as an enquiry and the references cited in the paper are treated as relevant answers.. The results in two tests show much better results than those achieved by the direct method.. No spurious material was retrieved by either method.. \n"
     ]
    }
   ],
   "source": [
    "count=1;\n",
    "for i in top_search:\n",
    "    k=str(i)\n",
    "    j=i+1\n",
    "    print(\"\\nsearch result :\",count,\"\\nDoc \",j,\" : \",processed_title[i],\"\\n\\n\")\n",
    "    print(doc_set[k])\n",
    "    count=count+1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
