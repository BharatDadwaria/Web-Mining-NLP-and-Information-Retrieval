{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Mining : Information Retrival and Natural Language Processing\n",
    "\n",
    "### Extracting the information from a document, performing Most Frequent bigram, Ranking them based on Matual Information and Part of speech."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Statment: \n",
    "Find the phrases/collocations from a text document such as news article, any article of social interest, any article on current topic a book chapter using following approaches:\n",
    "Take a text documents of your choice, show the result on atleast three documents\n",
    "Discuss the logic/formulae used and sample results in documentation along with analytical\n",
    "comments, highlight the actual keyphrases based on your/general knowledge.\n",
    "* a. Most frequent bigrams\n",
    "* b. Ranked on the basis of Mutual Information. Use standard formula for mutual information (Hint: find probability of combined occurrence of bigram and divide by their individual occurrence probability)\n",
    "* c. Noun Phrases based on the following POS patterns:\n",
    "    AN Linear Function\n",
    "\n",
    "        NN  regression coefficient\n",
    "\n",
    "        AAN Guassian random variable\n",
    "\n",
    "        ANN cumulative distribution function\n",
    "\n",
    "        NAN mean squared error\n",
    "        NPN degrees of freedom\n",
    "\n",
    "        NNN class probability function\n",
    "\n",
    "     ( Here A- Adjective , N- Noun, P- preposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Algorithm : \n",
    "* Step 1 : Importing all neccessary libraries, and text document.\n",
    "* Step 2 : Import the text file or document and preprocessing it (Removing panctuations, stop words etc).\n",
    "* step 3 : Creating functions of Most Frequent Bigrams and evaluating the most frequent bigrams\n",
    "* step 4 : Ranking them based on Mutual Information.\n",
    "* step 5 : Performing Port of speech tagging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk;\n",
    "import re;\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "       \n",
    "### 1. Most  frequent bigrams\n",
    "\n",
    "#### Importing the text/document file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file=open(\"How you can help Team-FTU.txt\",\"r\")\n",
    "data=file.readlines()\n",
    "#print(data)\n",
    "file.close()\n",
    "#i=0\n",
    "#for line in data:\n",
    "  #  print(i,\"\\t\",line)\n",
    "   # i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def lower_case(data):\n",
    "    \n",
    "    for i in range(len(data)):\n",
    "        data[i]=data[i].lower()\n",
    "    return data\n",
    "\n",
    "#Rather changing charectors, creating a list of these characters.\n",
    "def replace_panctuations(data):\n",
    "    \n",
    "    char_to_replace=['\\n',',','-','\"','(',')','.']\n",
    "    for i in range(len(data)):\n",
    "        for s in char_to_replace:\n",
    "            data[i]=data[i].replace(s,' ')\n",
    "    return data\n",
    "\n",
    "def preprocessing_data(data):\n",
    "    data=lower_case(data)\n",
    "    data=replace_panctuations(data)\n",
    "    return data;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "The text data is : \n",
      "\n",
      " ['the only available way to support team ftu is by the clicking ads on websites whenever you visit  this will help team ftu to keep community based websites alive and working  ', ' ', 'freetutorials eu ', 'freecoursesonline me ', 'discuss freetutorials us']\n",
      "\n",
      "\n",
      "The length of the unigrms are : \n",
      "\n",
      " 38\n",
      "\n",
      "\n",
      "Top Unigrams with their Frequency:\n",
      "\n",
      " [('freetutorials', 2), ('websites', 2), ('ftu', 2), ('team', 2), ('to', 2), ('the', 2), ('us', 1), ('discuss', 1), ('me', 1), ('freecoursesonline', 1)]\n",
      "<class 'zip'>\n",
      "\n",
      "\n",
      " The Top bi-Gram with their frequency: \n",
      "\n",
      " [(('team', 'ftu'), 2), (('freetutorials', 'us'), 1), (('discuss', 'freetutorials'), 1), (('me', 'discuss'), 1), (('freecoursesonline', 'me'), 1), (('eu', 'freecoursesonline'), 1), (('freetutorials', 'eu'), 1), (('working', 'freetutorials'), 1), (('and', 'working'), 1), (('alive', 'and'), 1)]\n"
     ]
    }
   ],
   "source": [
    "data=preprocessing_data(data)\n",
    "print('\\n\\nThe text data is : \\n\\n',data)\n",
    "length=len(data)\n",
    "text=\"\"\n",
    "#text=\" \".join(data)\n",
    "#or\n",
    "for line in data:\n",
    "    text+=line+\" \"\n",
    "gram=(text.split())\n",
    "print(\"\\n\\nThe length of the unigrms are : \\n\\n\",len(gram))\n",
    "\n",
    "\n",
    "def iterative_count(objec):\n",
    "    word_dict={}\n",
    "    for word in objec:\n",
    "        if word in word_dict:\n",
    "            word_dict[word]+=1\n",
    "        else:\n",
    "            word_dict[word]=1\n",
    "    return word_dict\n",
    "word_dict=iterative_count(gram)\n",
    "sorted_list=(sorted(word_dict.items(),key=lambda item: item[1]))\n",
    "top_k_grams=sorted_list[-1:-11:-1]\n",
    "print(\"\\n\\nTop Unigrams with their Frequency:\\n\\n\", top_k_grams)\n",
    "\n",
    "bi_gram=zip(gram,gram[1:])\n",
    "print(type(bi_gram))\n",
    "#for bi in bi_gram:\n",
    " #   print(bi)\n",
    "bi_gram_list= [item for item in bi_gram]\n",
    "#print((bi_gram_list))\n",
    "\n",
    "bigram_count=(iterative_count(bi_gram_list))\n",
    "\n",
    "sorted_list2=(sorted(bigram_count.items(), key=lambda item: item[1]))\n",
    "top_k_bigrams=sorted_list2[-1:-11:-1]\n",
    "print('\\n\\n The Top bi-Gram with their frequency: \\n\\n',top_k_bigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Ranked based on their Mutual Information :\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "The probability of ocuring each bigram : \n",
      "\n",
      " {'the only': 0.027777777777777776, 'only available': 0.027777777777777776, 'available way': 0.027777777777777776, 'way to': 0.027777777777777776, 'to support': 0.027777777777777776, 'support team': 0.027777777777777776, 'ftu is': 0.027777777777777776, 'is by': 0.027777777777777776, 'by the': 0.027777777777777776, 'the clicking': 0.027777777777777776, 'clicking ads': 0.027777777777777776, 'ads on': 0.027777777777777776, 'on websites': 0.027777777777777776, 'websites whenever': 0.027777777777777776, 'whenever you': 0.027777777777777776, 'you visit': 0.027777777777777776, 'visit this': 0.027777777777777776, 'this will': 0.027777777777777776, 'will help': 0.027777777777777776, 'help team': 0.027777777777777776, 'ftu to': 0.027777777777777776, 'to keep': 0.027777777777777776, 'keep community': 0.027777777777777776, 'community based': 0.027777777777777776, 'based websites': 0.027777777777777776, 'websites alive': 0.027777777777777776, 'alive and': 0.027777777777777776, 'and working': 0.027777777777777776, 'working freetutorials': 0.027777777777777776, 'freetutorials eu': 0.027777777777777776, 'eu freecoursesonline': 0.027777777777777776, 'freecoursesonline me': 0.027777777777777776, 'me discuss': 0.027777777777777776, 'discuss freetutorials': 0.027777777777777776, 'freetutorials us': 0.027777777777777776, 'team ftu': 0.05555555555555555}\n"
     ]
    }
   ],
   "source": [
    "#since we have the top bigrams \n",
    "#print(top_k_bigrams)\n",
    "\n",
    "#print(top_k_bigrams[1][1])\n",
    "#print(sorted_list2)\n",
    "#print(len(sorted_list2))\n",
    "prob_bigrams=list(sorted_list2)\n",
    "#prob_bigrams[1][1]=2\n",
    "prob_of_bigrams={}\n",
    "keys=list()\n",
    "for i in range (len(prob_bigrams)):\n",
    "    val=prob_bigrams[i][0][0]+str(' ')+prob_bigrams[i][0][1]\n",
    "    keys.append(val)\n",
    "    prob=prob_bigrams[i][1]/len(prob_bigrams)\n",
    "    prob_of_bigrams[val]= prob \n",
    "print(\"\\n\\nThe probability of ocuring each bigram : \\n\\n\",prob_of_bigrams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "The Probability of occuring of each word: \n",
      "\n",
      " {'only': 0.03125, 'available': 0.03125, 'way': 0.03125, 'support': 0.03125, 'is': 0.03125, 'by': 0.03125, 'clicking': 0.03125, 'ads': 0.03125, 'on': 0.03125, 'whenever': 0.03125, 'you': 0.03125, 'visit': 0.03125, 'this': 0.03125, 'will': 0.03125, 'help': 0.03125, 'keep': 0.03125, 'community': 0.03125, 'based': 0.03125, 'alive': 0.03125, 'and': 0.03125, 'working': 0.03125, 'eu': 0.03125, 'freecoursesonline': 0.03125, 'me': 0.03125, 'discuss': 0.03125, 'us': 0.03125, 'the': 0.0625, 'to': 0.0625, 'team': 0.0625, 'ftu': 0.0625, 'websites': 0.0625, 'freetutorials': 0.0625}\n"
     ]
    }
   ],
   "source": [
    "#print(sorted_list)\n",
    "#print(sorted_list[0][0])\n",
    "prob_of_ugrams={}\n",
    "for i in range(len(sorted_list)):\n",
    "    val=sorted_list[i][0]\n",
    "    prob=sorted_list[i][1]/len(sorted_list)\n",
    "    prob_of_ugrams[sorted_list[i][0]]=prob\n",
    "    \n",
    "print(\"\\n\\nThe Probability of occuring of each word: \\n\\n\",prob_of_ugrams)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('me discuss', 28.444444444444443), ('freecoursesonline me', 28.444444444444443), ('eu freecoursesonline', 28.444444444444443), ('and working', 28.444444444444443), ('alive and', 28.444444444444443), ('community based', 28.444444444444443), ('keep community', 28.444444444444443), ('will help', 28.444444444444443), ('this will', 28.444444444444443), ('visit this', 28.444444444444443)]\n"
     ]
    }
   ],
   "source": [
    "#Mutual information : \n",
    "mutual_information={}\n",
    "mutual_info=prob_of_bigrams\n",
    "key_list=list(prob_of_bigrams.keys())\n",
    "#print(key_list)\n",
    "\n",
    "#first_part2=list(prob_of_bigrams)[0]\n",
    "#print(first_part2)\n",
    "#print(word_dict)\n",
    "#print((prob_of_bigrams.items(),key=lambda item: item[1])))\n",
    "#print(prob_of_bigrams.get('only avialble'))\n",
    "#print(prob_of_bigrams.get('only available')\n",
    "#print(prob_of_bigrams[first_part])\n",
    "for i in range(len(sorted_list2)):\n",
    "    bi_val=key_list[i]\n",
    "    uni_val=bi_val.split()\n",
    "    mut_info=prob_of_bigrams.get(bi_val)/(prob_of_ugrams.get(uni_val[0])*prob_of_ugrams.get(uni_val[1]))\n",
    "    mutual_information[bi_val]=mut_info    \n",
    "#print(\"Mutual information of the gram is : \\n\",mutual_information)\n",
    "\n",
    "sorted_mutual_info=(sorted(mutual_information.items(),key=lambda item: item[1]))\n",
    "#print(sorted_mutual_info)\n",
    "top_k_mutual_info=sorted_mutual_info[-1:-11:-1]\n",
    "print(top_k_mutual_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Noun Phrases based on the following POS patterns:\n",
    "\t\n",
    "\n",
    "       AN   ->   Linear Function\n",
    "\n",
    "       NN\t->   regression coefficient\n",
    "\n",
    "       AAN\t->   Guassian random variable\n",
    "\n",
    "       ANN\t->   cumulative distribution function\n",
    "\n",
    "       NAN\t->   mean squared error\n",
    "\n",
    "       NPN\t->\t degrees of freedom\n",
    "\n",
    "       NNN  ->   class probability function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Linear', 'Function', 'regression', 'coefficient', 'Guassian', 'random', 'variable', 'cumulative', 'distribution', 'function', 'mean', 'squared', 'error', 'degrees', 'of', 'freedom', 'class', 'probability', 'function']\n",
      "[('Linear', 'JJ'), ('Function', 'NNP'), ('regression', 'NN'), ('coefficient', 'NN'), ('Guassian', 'JJ'), ('random', 'NN'), ('variable', 'JJ'), ('cumulative', 'JJ'), ('distribution', 'NN'), ('function', 'NN'), ('mean', 'NN'), ('squared', 'VBD'), ('error', 'NN'), ('degrees', 'NNS'), ('of', 'IN'), ('freedom', 'NN'), ('class', 'NN'), ('probability', 'NN'), ('function', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "text='Linear Function regression coefficient Guassian random variable cumulative distribution function mean squared error degrees of freedom class probability function'.split()\n",
    "print(text)\n",
    "token_tag=nltk.pos_tag(text)\n",
    "print(token_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<JJ|JJR|JJS><NN|NNS|NNP>'>\n",
      "After Chunking\n",
      " (S\n",
      "  (Chunk Linear/JJ Function/NNP)\n",
      "  regression/NN\n",
      "  coefficient/NN\n",
      "  (Chunk Guassian/JJ random/NN)\n",
      "  variable/JJ\n",
      "  (Chunk cumulative/JJ distribution/NN)\n",
      "  function/NN\n",
      "  mean/NN\n",
      "  squared/VBD\n",
      "  error/NN\n",
      "  degrees/NNS\n",
      "  of/IN\n",
      "  freedom/NN\n",
      "  class/NN\n",
      "  probability/NN\n",
      "  function/NN)\n"
     ]
    }
   ],
   "source": [
    "#For AN \n",
    "chunk = r\"\"\"Chunk: {<JJ|JJR|JJS><NN|NNS|NNP>}\"\"\" \n",
    "chunker=nltk.RegexpParser(chunk)\n",
    "print(chunker)\n",
    "output=chunker.parse(token_tag)\n",
    "output.draw()\n",
    "print('After Chunking\\n',output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN|NNS|NNP><NN|NNS|NNP>'>\n",
      "After chunking (S\n",
      "  Linear/JJ\n",
      "  (Chunk Function/NNP regression/NN)\n",
      "  coefficient/NN\n",
      "  Guassian/JJ\n",
      "  random/NN\n",
      "  variable/JJ\n",
      "  cumulative/JJ\n",
      "  (Chunk distribution/NN function/NN)\n",
      "  mean/NN\n",
      "  squared/VBD\n",
      "  (Chunk error/NN degrees/NNS)\n",
      "  of/IN\n",
      "  (Chunk freedom/NN class/NN)\n",
      "  (Chunk probability/NN function/NN))\n"
     ]
    }
   ],
   "source": [
    "#for NN\n",
    "chunk=r\"\"\"Chunk: {<NN|NNS|NNP><NN|NNS|NNP>}\"\"\"\n",
    "chunker=nltk.RegexpParser(chunk)\n",
    "print(chunker)\n",
    "output=chunker.parse(token_tag)\n",
    "print('After chunking',output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<JJ|JJR|JJS><NN|NNS|NNP><NN|NNS|NNP>'>\n",
      "After chunking (S\n",
      "  (Chunk Linear/JJ Function/NNP regression/NN)\n",
      "  coefficient/NN\n",
      "  Guassian/JJ\n",
      "  random/NN\n",
      "  variable/JJ\n",
      "  (Chunk cumulative/JJ distribution/NN function/NN)\n",
      "  mean/NN\n",
      "  squared/VBD\n",
      "  error/NN\n",
      "  degrees/NNS\n",
      "  of/IN\n",
      "  freedom/NN\n",
      "  class/NN\n",
      "  probability/NN\n",
      "  function/NN)\n"
     ]
    }
   ],
   "source": [
    "#for ANN\n",
    "chunk=r\"\"\"Chunk: {<JJ|JJR|JJS><NN|NNS|NNP><NN|NNS|NNP>}\"\"\"\n",
    "chunker=nltk.RegexpParser(chunk)\n",
    "print(chunker)\n",
    "output=chunker.parse(token_tag)\n",
    "print('After chunking', output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'chunker_NAN' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-e51310526bd4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mchunk\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;34mr\"\"\"Chunk: {<NN|NNS|NNP><JJ|JJR|JJS><NN|NNS|NNP>}\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mchunker\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRegexpParser\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunker_NAN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mchunker\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken_tag\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"After chunking\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'chunker_NAN' is not defined"
     ]
    }
   ],
   "source": [
    "#for NAN\n",
    "chunk= r\"\"\"Chunk: {<NN|NNS|NNP><JJ|JJR|JJS><NN|NNS|NNP>}\"\"\" \n",
    "chunker=nltk.RegexpParser(chunk)\n",
    "print(chunker_NAN)\n",
    "output=chunker.parse(token_tag)\n",
    "print(\"After chunking\",output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN|NNS|NNP><IN><NN|NNS|NNP>'>\n",
      "After chunking (S\n",
      "  Linear/JJ\n",
      "  Function/NNP\n",
      "  regression/NN\n",
      "  coefficient/NN\n",
      "  Guassian/JJ\n",
      "  random/NN\n",
      "  variable/JJ\n",
      "  cumulative/JJ\n",
      "  distribution/NN\n",
      "  function/NN\n",
      "  mean/NN\n",
      "  squared/VBD\n",
      "  error/NN\n",
      "  (Chunk degrees/NNS of/IN freedom/NN)\n",
      "  class/NN\n",
      "  probability/NN\n",
      "  function/NN)\n"
     ]
    }
   ],
   "source": [
    "#for NPN\n",
    "chunk= r\"\"\"Chunk: {<NN|NNS|NNP><IN><NN|NNS|NNP>}\"\"\" \n",
    "chunker=nltk.RegexpParser(chunk)\n",
    "print(chunker)\n",
    "output = chunker.parse(token_tag)\n",
    "print(\"After chunking\",output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN|NNS|NNP><NN|NNS|NNP><NN|NNS|NNP>'>\n",
      "After chunking (S\n",
      "  Linear/JJ\n",
      "  (Chunk Function/NNP regression/NN coefficient/NN)\n",
      "  Guassian/JJ\n",
      "  random/NN\n",
      "  variable/JJ\n",
      "  cumulative/JJ\n",
      "  (Chunk distribution/NN function/NN mean/NN)\n",
      "  squared/VBD\n",
      "  error/NN\n",
      "  degrees/NNS\n",
      "  of/IN\n",
      "  (Chunk freedom/NN class/NN probability/NN)\n",
      "  function/NN)\n"
     ]
    }
   ],
   "source": [
    "#for NNN\n",
    "chunk= r\"\"\"Chunk: {<NN|NNS|NNP><NN|NNS|NNP><NN|NNS|NNP>}\"\"\" \n",
    "chunker=nltk.RegexpParser(chunk)\n",
    "print(chunker)\n",
    "output=chunker.parse(token_tag)\n",
    "print(\"After chunking\",output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
